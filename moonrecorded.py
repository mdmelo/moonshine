# --- moonshine --- #

import sys
import time
import wave

from pathlib import Path
import tokenizers
import keras
import onnx
import numpy as np


from moonshine import load_model, Moonshine
from moonshine_onnx import MoonshineOnnxModel

_benchmark = True
transcribe = None

ASSETS_DIR = "./moonshine/assets/"
DEFAULT_AUDIO_FILE = "./moonshine/assets/beckett.wav"


def load_audio(audio, return_numpy=False):
    if isinstance(audio, (str, Path)):
        import librosa

        audio, _ = librosa.load(audio, sr=16_000)
        if return_numpy:
            return audio[None, ...]
        audio = keras.ops.expand_dims(keras.ops.convert_to_tensor(audio), 0)
    return audio


def assert_audio_size(audio):
    assert len(keras.ops.shape(audio)) == 2, "audio should be of shape [batch, samples]"
    num_seconds = keras.ops.convert_to_numpy(keras.ops.size(audio) / 16_000)
    assert (
        0.1 < num_seconds < 64
        ), "Moonshine models support audio segments that are between 0.1s and 64s in a single transcribe call. For transcribing longer segments, pre-segment your audio and provide shorter segments."
    return num_seconds


def transcribe(audio, model_name, model_dir):

    if model_dir is not None:
        # use saved onnx model. loads a serialized onnx ModelProto into memory,
        # then checks the consistency of the models after load.
        # if model_dir.endswith("/") == False:
        #     model_dir = model_dir + "/"
        #
        # preprocess = onnx.load_model(model_dir + "preprocess.onnx")
        # onnx.checker.check_model(preprocess)
        #
        # encode = onnx.load_model(model_dir + "encode.onnx")
        # onnx.checker.check_model(encode)
        #
        # cached_decode = onnx.load_model(model_dir + "cached_decode.onnx")
        # onnx.checker.check_model(cached_decode)
        #
        # uncached_decode = onnx.load_model(model_dir + "uncached_decode.onnx")
        # onnx.checker.check_model(uncached_decode)
        model = MoonshineOnnxModel(models_dir=model_dir)
        print("model loaded locally...")

        with wave.open(audio) as f:
            params = f.getparams()
            assert (
                    params.nchannels == 1
                    and params.framerate == 16_000
                    and params.sampwidth == 2
                    ), f"wave file should have 1 channel, 16KHz, and int16"
            audio = f.readframes(params.nframes)
            audio = np.frombuffer(audio, np.int16) / 32768.0
            audio = audio.astype(np.float32)[None, ...]

    elif model_name is not None:
        # download model from HF
        model = load_model(model_name)
        print("model downloaded...")

        audio = load_audio(audio)
        num_seconds = assert_audio_size(audio)
        print ("loaded {} seconds of recorded audio".format(num_seconds))

    else:
        print("missing model name/dir")
        sys.exit(1)

    # see misc/moonshine/model.py.  Using keras, create the model's audio preprocessor,
    # encoder, and decorder. keras converts the audio samples to a tensor, then feeds
    # to model yielding token IDs.
    tokens = model.generate(audio)
    print("tokens generated by model...")

    # see https://github.com/huggingface/tokenizers ...bindings/python/src/tokenizer.rs
    # decodes a batch of ids (from a language model) back to their
    # corresponding strings
    return load_tokenizer().decode_batch(tokens)


def load_tokenizer():
    tokenizer_file = ASSETS_DIR + "tokenizer.json"
    # return a Tokenizer using local JSON file containing a vocabulary
    tokenizer = tokenizers.Tokenizer.from_file(str(tokenizer_file))
    return tokenizer


def moonshine_main_recorded_audio(audio, model_name, model_dir, cbfunc=None):

    if _benchmark:
        model = load_model("moonshine/" + model_name.split("/")[-1])
        print("model loaded from HF...")

        audio = load_audio(audio)
        num_seconds = assert_audio_size(audio)
        print ("{} seconds recorded audio".format(num_seconds))

        # Warming up...
        for _ in range(4):
            _ = model.generate(audio)

        # Benchmarking...
        N = 8
        start_time = time.time_ns()
        for _ in range(N):
            _ = model.generate(audio)
        end_time = time.time_ns()

        elapsed_time = (end_time - start_time) / N
        elapsed_time /= 1e6

        print(f"Time to transcribe {num_seconds:.2f}s of speech is {elapsed_time:.2f}ms")

    else:
        start_time = time.time_ns()
        text = transcribe(audio, model_name=model_name, model_dir=model_dir)
        elapsed_time = (time.time_ns() - start_time)
        elapsed_time /= 1e6

        print("text: |{}|.\nelapsed: {:.2f} ms".format(text, elapsed_time))



if __name__ == "__main__":
    # /files/pyASR/test16000mono_two.wav
    if len(sys.argv) > 1:
        recorded_audio = sys.argv[1]
    else:
        recorded_audio = DEFAULT_AUDIO_FILE
    print("using recorded audio from ", recorded_audio)

    moonshine_main_recorded_audio(recorded_audio, "base", "./models/base")
